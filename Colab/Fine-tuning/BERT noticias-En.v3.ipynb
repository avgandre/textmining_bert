{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT noticias-En.v3.ipynb","provenance":[{"file_id":"1bBUl2PzKUQhxtSMWT5dBBP1TfUXTPiGn","timestamp":1648151540959},{"file_id":"1fdrMllG1VFwVkJHG0Vga6lWrgku7BkH7","timestamp":1635278334779},{"file_id":"1j-THdgLf_XOhZvZ0auTN6i9CIARPrvsl","timestamp":1635210983576},{"file_id":"1R7fLhT28aLu4M7CJuIblmqrnO6gxMZFd","timestamp":1634921059710},{"file_id":"1Qz6QIcAsaFKlHcJtwcGZhmRTqMg8hz8e","timestamp":1634901366570}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2E5dZcv_mv5t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648416192931,"user_tz":180,"elapsed":15025,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}},"outputId":"29e0fc9c-1bad-4cff-82a1-86489c09bc7b"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"KbF_ZxV5mzfG","executionInfo":{"status":"ok","timestamp":1648416203526,"user_tz":180,"elapsed":10630,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import time\n","import re\n","from random import sample\n","\n","from sklearn.model_selection import train_test_split"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RIluI2lo0D6","executionInfo":{"status":"ok","timestamp":1648416203877,"user_tz":180,"elapsed":417,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["#Tempo de processamento\n","tempoInicial = time.time()\n","\n","#Faz a leitura da base\n","df = pd.read_csv('dataset7DiasCompleto-En.v1.csv', sep=';')\n","df.describe()\n","\n","colunaCorpus='titulo_processado'\n","colunaResultado='classe'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"qq_ROpFgo-CH","executionInfo":{"status":"ok","timestamp":1648416203880,"user_tz":180,"elapsed":15,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["randomState = sample(range(0, 1000), 1)[0];\n","\n","#Definindo X, y\n","X = df[df.columns.difference([colunaResultado])]\n","y = df[colunaResultado]\n","\n","#Separa base treinamento e teste\n","XTreino, XTeste, yTreino, yTeste = train_test_split(X, y, train_size=0.7, \n","                                                    stratify=y, shuffle=True, \n","                                                    random_state=randomState)\n","XTreino = XTreino[colunaCorpus].values\n","yTreino = yTreino.values\n","XTeste = XTeste[colunaCorpus].values\n","yTeste = yTeste.values"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"TO984qUu6EJE","executionInfo":{"status":"ok","timestamp":1648416205027,"user_tz":180,"elapsed":1160,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["import transformers\n","\n","## distil-bert tokenizer\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSg_B1tGpBoX","executionInfo":{"status":"ok","timestamp":1648416205029,"user_tz":180,"elapsed":16,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["corpus = XTreino\n","#corpus = XTreino[colunaCorpus]\n","maxlen = 512"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y012tkgpXIT0","executionInfo":{"status":"ok","timestamp":1648416210027,"user_tz":180,"elapsed":5013,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}},"outputId":"a5532750-5083-4b74-aafb-221fcdd46250"},"source":["def tokenize(sentences, tokenizer):\n","    input_ids, input_masks, input_segments = [],[],[]\n","    for s in range(len(sentences)):\n","        sentence = sentences[s]\n","        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=512, pad_to_max_length=True, \n","                                             return_attention_mask=True, return_token_type_ids=True)\n","        input_ids.append(inputs['input_ids'])\n","        input_masks.append(inputs['attention_mask'])\n","        input_segments.append(inputs['token_type_ids'])        \n","        \n","    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')\n","\n","X = tokenize(corpus, tokenizer)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_sisVlW6mmF","executionInfo":{"status":"ok","timestamp":1648416238513,"user_tz":180,"elapsed":28549,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}},"outputId":"4792002b-6b84-4497-8e8e-d3b8f5536599"},"source":["import tensorflow as tf\n","from transformers import TFBertModel\n","\n","## inputs\n","input_ids_in = tf.keras.layers.Input(shape=(512,), name='input_token', dtype='int32')\n","input_masks_in = tf.keras.layers.Input(shape=(512,), name='masked_token', dtype='int32') \n","\n","## pre-trained bert with config\n","transformer_model = TFBertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = False)\n","bert_out = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n","\n","## fine-tuning\n","x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(bert_out)\n","x = tf.keras.layers.GlobalMaxPool1D()(x)\n","x = tf.keras.layers.Dense(50, activation='relu')(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","x = tf.keras.layers.Dense(len(np.unique(yTreino)), activation='sigmoid')(x)\n","model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = x)\n","\n","for layer in model.layers[:3]:\n","    layer.trainable = False\n","\n","model.compile(loss='sparse_categorical_crossentropy', \n","              optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_token (InputLayer)       [(None, 512)]        0           []                               \n","                                                                                                  \n"," masked_token (InputLayer)      [(None, 512)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_token[0][0]',            \n","                                thPoolingAndCrossAt               'masked_token[0][0]']           \n","                                tentions(last_hidde                                               \n","                                n_state=(None, 512,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 512, 200)     695200      ['tf_bert_model[0][0]']          \n","                                                                                                  \n"," global_max_pooling1d (GlobalMa  (None, 200)         0           ['bidirectional[0][0]']          \n"," xPooling1D)                                                                                      \n","                                                                                                  \n"," dense (Dense)                  (None, 50)           10050       ['global_max_pooling1d[0][0]']   \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 50)           0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 5)            255         ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 110,187,745\n","Trainable params: 705,505\n","Non-trainable params: 109,482,240\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"A7aBe0r5972d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08299fb6-c16d-470a-f23b-d833b696c958","executionInfo":{"status":"ok","timestamp":1648429901746,"user_tz":180,"elapsed":13663243,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["## encode y\n","#dic_y_mapping = {n:label for n,label in \n","#                 enumerate(np.unique(yTreino))}\n","#inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n","#yTreino = np.array([inverse_dic[y] for y in yTreino])\n","\n","## train\n","training = model.fit(x=X, y=yTreino, batch_size=128, epochs=1, shuffle=True, \n","                     verbose=1, validation_split=0.2)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["38/38 [==============================] - 13616s 359s/step - loss: 0.8348 - accuracy: 0.7064 - val_loss: 0.5442 - val_accuracy: 0.8025\n"]}]},{"cell_type":"code","metadata":{"id":"BVWo4h_sMdbc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648429903124,"user_tz":180,"elapsed":1384,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}},"outputId":"d337073a-5fcf-4d21-e551-50b136ae690e"},"source":["corpus = XTeste\n","X = tokenize(corpus, tokenizer)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"UavXNHicMfCn","executionInfo":{"status":"ok","timestamp":1648434470958,"user_tz":180,"elapsed":4567838,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}}},"source":["## test\n","predicted_prob = model.predict(X)\n","predicted = [np.argmax(pred) for pred in predicted_prob]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2hjEq4WMhor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648434470959,"user_tz":180,"elapsed":34,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}},"outputId":"1d641af9-5399-4bf0-da8d-672340f88da2"},"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(yTeste, predicted, labels=[0, 1, 2, 3, 4]))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.90      0.82       452\n","           1       0.80      0.76      0.78       629\n","           2       0.86      0.78      0.82       645\n","           3       0.80      0.81      0.80       600\n","           4       0.76      0.78      0.77       268\n","\n","    accuracy                           0.80      2594\n","   macro avg       0.80      0.80      0.80      2594\n","weighted avg       0.80      0.80      0.80      2594\n","\n"]}]},{"cell_type":"code","source":["print(\"\\n--- %.2f minutos ---\" % ((time.time() - tempoInicial) / 60))"],"metadata":{"id":"mkgJ-cpjGV6p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648434470961,"user_tz":180,"elapsed":29,"user":{"displayName":"André Vinícius Gonçalves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYQ2yHMybauoyXTTOgRZEVKWq7grg4lMJnyOFLw=s64","userId":"08895061883415001309"}},"outputId":"3925feb3-34fb-4d26-8b5e-01a62d6b9c44"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- 304.46 minutos ---\n"]}]}]}